{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f3a7ff6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a25fa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 569\n",
      "Test size : 143\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# ===============================\n",
    "# TRAIN / TEST SPLIT (80 / 20)\n",
    "# ===============================\n",
    "random.seed(42)\n",
    "\n",
    "indices = list(range(len(X)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "split_idx = int(0.8 * len(indices))\n",
    "\n",
    "train_idx = indices[:split_idx]\n",
    "test_idx  = indices[split_idx:]\n",
    "\n",
    "X_train = [X[i] for i in train_idx]\n",
    "y_train = [y[i] for i in train_idx]\n",
    "\n",
    "X_test = [X[i] for i in test_idx]\n",
    "y_test = [y[i] for i in test_idx]\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Test size :\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9ea6b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means: [-0.01819094427569709, 6.670096280728936e-05, -0.030146853412043317, -0.0254946749221174, -0.00028917637070838465, 0.01784400504652037, -0.005541074762952938]\n",
      "Stds : [1.0016074081481439, 1.0000188864182753, 0.9876400712252256, 1.0000286268431353, 1.0156419357139692, 1.0491042361967042, 0.994998472526364]\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# HELPER FUNCTIONS\n",
    "# ===============================\n",
    "\n",
    "def find_mean(dataset):\n",
    "    return sum(dataset) / len(dataset)\n",
    "\n",
    "def find_standard_deviation(dataset, mean):\n",
    "    return (sum((x - mean) ** 2 for x in dataset) / len(dataset)) ** 0.5\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# COMPUTE MEANS & STDS (TRAIN ONLY)\n",
    "# ===============================\n",
    "\n",
    "feature_means = []\n",
    "feature_stds = []\n",
    "\n",
    "for feature_idx in range(n_features):\n",
    "    column_values = [row[feature_idx] for row in X_train]\n",
    "\n",
    "    mean = find_mean(column_values)\n",
    "    std = find_standard_deviation(column_values, mean)\n",
    "\n",
    "    if std == 0:\n",
    "        std = 1.0\n",
    "\n",
    "    feature_means.append(mean)\n",
    "    feature_stds.append(std)\n",
    "\n",
    "print(\"Means:\", feature_means)\n",
    "print(\"Stds :\", feature_stds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a00ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1000\n",
      "Epoch 2000\n",
      "Epoch 3000\n",
      "Epoch 4000\n",
      "Epoch 5000\n",
      "Epoch 6000\n",
      "Epoch 7000\n",
      "Epoch 8000\n",
      "Epoch 9000\n",
      "Epoch 10000\n",
      "Epoch 11000\n",
      "Epoch 12000\n",
      "Epoch 13000\n",
      "Epoch 14000\n",
      "\n",
      "Trained weights: [-1.039574018158522, 1.2640157063505881, -0.6227957188980372, -0.29810272188808645, -0.03452107106915563, 0.10383542709986539, 0.03590202166911438]\n",
      "Trained bias: -0.5354805217899608\n",
      "\n",
      "Test predictions (first 5):\n",
      "Sample 0: probability=0.6430, predicted=1, actual=1\n",
      "Sample 1: probability=0.1340, predicted=0, actual=0\n",
      "Sample 2: probability=0.9111, predicted=1, actual=1\n",
      "Sample 3: probability=0.4471, predicted=0, actual=0\n",
      "Sample 4: probability=0.1391, predicted=0, actual=1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "# ===============================\n",
    "# HYPERPARAMETERS\n",
    "# ===============================\n",
    "learning_rate = 0.005\n",
    "lambda_ = 0.0001\n",
    "epochs = 15000\n",
    "\n",
    "# ===============================\n",
    "# DATA SIZES\n",
    "# ===============================\n",
    "n_samples = len(X_train)\n",
    "n_features = len(X_train[0])\n",
    "\n",
    "# ===============================\n",
    "# INITIALIZE PARAMETERS\n",
    "# ===============================\n",
    "weight = [random.uniform(-0.01, 0.01) for _ in range(n_features)]\n",
    "b = 0.0\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# MODEL FUNCTIONS\n",
    "# ===============================\n",
    "def sigmoid(z):\n",
    "    if z >= 0:\n",
    "        return 1 / (1 + math.exp(-z))\n",
    "    else:\n",
    "        ez = math.exp(z)\n",
    "        return ez / (1 + ez)\n",
    "\n",
    "\n",
    "def find_weighted_sum_for_row(row):\n",
    "    return sum(weight[i] * row[i] for i in range(n_features)) + b\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# TRAINING\n",
    "# ===============================\n",
    "def train_one_epoch():\n",
    "    global b\n",
    "\n",
    "    preds = [\n",
    "        sigmoid(find_weighted_sum_for_row(X_train[j]))\n",
    "        for j in range(n_samples)\n",
    "    ]\n",
    "\n",
    "    # update weights\n",
    "    for i in range(n_features):\n",
    "        grad = sum(\n",
    "            (preds[j] - y_train[j]) * X_train[j][i]\n",
    "            for j in range(n_samples)\n",
    "        ) / n_samples\n",
    "\n",
    "        # L2 regularization (no bias)\n",
    "        grad += lambda_ * weight[i]\n",
    "\n",
    "        weight[i] -= learning_rate * grad\n",
    "\n",
    "    # update bias\n",
    "    b_grad = sum(preds[j] - y_train[j] for j in range(n_samples)) / n_samples\n",
    "    b -= learning_rate * b_grad\n",
    "\n",
    "\n",
    "def train_model(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        train_one_epoch()\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# TRAIN\n",
    "# ===============================\n",
    "train_model(epochs)\n",
    "\n",
    "print(\"\\nTrained weights:\", weight)\n",
    "print(\"Trained bias:\", b)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# TEST ON TEST SET\n",
    "# ===============================\n",
    "print(\"\\nTest predictions (first 5):\")\n",
    "for i in range(5):\n",
    "    p = sigmoid(find_weighted_sum_for_row(X_test[i]))\n",
    "    print(f\"Sample {i}: probability={p:.4f}, predicted={1 if p>=0.5 else 0}, actual={y_test[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed8aa12",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e53a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daac373c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b99abe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
